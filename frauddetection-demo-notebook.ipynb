{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frauddetection demo notebook\n",
    "This notebook is a fork of the [frauddetection-notebook-template](https://gitlab.com/opendatahub/fraud-detection-tutorial/blob/master/jupyterhub/frauddetection-notebook-template.ipynb).\n",
    "The data originates from [Kaggle/ULB](https://www.kaggle.com/mlg-ulb/creditcardfraud).\n",
    "\n",
    "## The demo contains following steps:\n",
    "- Start a spark session\n",
    "- Load training data from Ceph/S3 into Spark\n",
    "- Train and save a model\n",
    "- Show accuracy and confusion matrix\n",
    "- Upload the model to Ceph/S3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a spark session and load training data from S3 into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting a spark session ...\n",
      "Spark version: 2.2.1\n",
      "Spark Session Success: ['jupyterhub-nb-kube-3aadmin']\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pyspark\n",
    "import time\n",
    "import os\n",
    "import socket\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Getting a spark session ...\")\n",
    "        \n",
    "\n",
    "# Add the necessary Hadoop and AWS jars to access Ceph from Spark\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.jars.ivy=/tmp/.ivy2 --packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell'\n",
    "\n",
    "spark = SparkSession.builder.master('local[2]').getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "\n",
    "print(\"Spark version:\", spark.sparkContext.version)\n",
    "print(\"Spark Session Success:\", spark.range(5, numPartitions=5).rdd.map(lambda x: socket.gethostname()).distinct().collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Ceph/S3 paramters and test connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects in bucket: fraud-demo\n",
      "creditcard-sample10k.csv\n",
      "creditcard.csv\n"
     ]
    }
   ],
   "source": [
    "s3_region = '' # fill in for AWS, blank for Ceph\n",
    "\n",
    "s3_endpoint_url= 'http://10.32.111.87:8000' # Change to your S3 URL\n",
    "s3_access_key = os.environ['AWS_ACCESS_KEY_ID'] \n",
    "s3_secret_key = os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "s3_bucket = 'fraud-demo'\n",
    "training_data_file = \"creditcard-sample10k.csv\"\n",
    "\n",
    "s3 = boto3.client(service_name='s3',aws_access_key_id = s3_access_key,aws_secret_access_key = s3_secret_key, endpoint_url=s3_endpoint_url)\n",
    "\n",
    "\n",
    "test_s3 = True\n",
    "if test_s3:\n",
    "   result = s3.list_objects(Bucket=s3_bucket)\n",
    "   print(\"Objects in bucket:\", s3_bucket)\n",
    "   for item in result['Contents']:\n",
    "     print(item['Key'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data from S3 into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load training data from S3 into Spark ...\n",
      "Spark reading transaction data ...\n",
      "Total number of credit card transaction rows: 10000\n",
      "Total number of rows with fraud 38\n"
     ]
    }
   ],
   "source": [
    "print(\"Load training data from S3 into Spark ...\")\n",
    "\n",
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", s3_endpoint_url)\n",
    "hadoopConf.set(\"fs.s3a.access.key\", s3_access_key)\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", s3_secret_key)\n",
    "hadoopConf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"false\") # false if not https\n",
    "\n",
    "\n",
    "print(\"Spark reading transaction data ...\")\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"True\").option(\"mode\", \"DROPMALFORMED\").load(f\"s3a://{s3_bucket}/{training_data_file}\")\n",
    "\n",
    "print(\"Total number of credit card transaction rows: %d\" % df.count())\n",
    "### Check the total number of rows with fraud is detected\n",
    "print(\"Total number of rows with fraud\", df[(df['Class']==1)].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Sklearn Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toyal samples: 10000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "#Order the credit card transaction by transaction time\n",
    "df.orderBy(\"Time\")\n",
    "\n",
    "#number of rows in the dataset\n",
    "n_samples = df.count()\n",
    "print(\"Toyal samples:\", n_samples)\n",
    "\n",
    "#Split into train and test\n",
    "train_size = 0.75\n",
    "\n",
    "train_limit = int(n_samples * train_size)\n",
    "df_train = df.limit(train_limit)     \n",
    "df_test = df.subtract(df_train) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train transactions: 7500\n",
      "Number of test  transactions: 2500\n",
      "Original Data Schema\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Time: integer (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Number of train transactions:', df_train.count())\n",
    "print('Number of test  transactions:', df_test.count())\n",
    "#Data Schema\n",
    "print(\"Original Data Schema\")\n",
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score:  0.9992\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Define features and target variables for convenience.\n",
    "drop_time_class = ['_c0', 'Time', 'Class']\n",
    "drop_class=['Class']\n",
    "\n",
    "#Create Train Datasets\n",
    "features_train = df_train.drop(*drop_time_class)\n",
    "target_train = df_train.select(\"Class\")\n",
    "\n",
    "#Create Test Datasets\n",
    "features_test = df_test.drop(*drop_time_class)\n",
    "target_test = df_test.select(\"Class\")\n",
    "\n",
    "#Create a RandomForest Classifier mode\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=4, n_jobs=10)\n",
    "\n",
    "#Convert to pandas\n",
    "features_test_pd = features_test.toPandas()\n",
    "target_test_pd = target_test.toPandas()\n",
    "\n",
    "features_train_pd = features_train.toPandas()\n",
    "target_train_pd = target_train.toPandas()\n",
    "\n",
    "model.fit(features_train_pd, target_train_pd.values.ravel())\n",
    "\n",
    "pred_train = model.predict(features_train_pd)\n",
    "pred_test = model.predict(features_test_pd)\n",
    "\n",
    "pred_train_prob = model.predict_proba(features_train_pd)\n",
    "pred_test_prob = model.predict_proba(features_test_pd)\n",
    "\n",
    "\n",
    "score = accuracy_score(target_test_pd, model.predict(features_test_pd))\n",
    "print(\"Model accuracy score: \",score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'model.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/app-root/lib/python3.6/site-packages (3.1.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/app-root/lib/python3.6/site-packages (from matplotlib) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/app-root/lib/python3.6/site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/app-root/lib/python3.6/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/app-root/lib/python3.6/site-packages (from matplotlib) (1.17.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/app-root/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix test set:\n",
      "[[2487    0]\n",
      " [   0   13]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlcAAAFACAYAAACRAFk6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAQKElEQVR4nO3de8xkdX3H8c93n2Vhl0UDgnLvAoILrYrXaq2ppLRQq2IttnhLamrxfrdVo6lNY2tNaFpNqhYvaVKtNJK2sdZL1dJWqcpN5bbuqqAgIIgo4g0W+PWPZ8DlkYVd/c7OzsPrlWyYOXPmzPchT3bfc86ZMzXGCAAAPVbMegAAgOVEXAEANBJXAACNxBUAQCNxBQDQSFwBADQSVwAAjcQVAECjlbMeALZFVa1PckKSAyaLrkjywTHGhtlNBQA/zZ4rdnpV9eokpyWpJGdN/lSS91fVa2Y5G7A8VdWzZz0D86t8/Q07u6ralOQXxxiblyxfleSiMcbhs5kMWK6q6rIxxsGznoP55LAg8+DWJPsn+fqS5ftNHgPYblV1/tYeSnK/HTkLy4u4Yh68LMknq+rLSS6fLDs4yf2TvGhmUwHz7n5JjkvynSXLK8n/7fhxWC7EFTu9McZHq+qIJI/MHU9oP3uMccvsJgPm3IeSrB1jfGHpA1X13zt+HJYL51wBADTyaUF2ClV1UFWdUVUXV9VFVfXSJY+/sqpGVe09uX/vqvr3qvriZP1nT5YfU1Vf2OLPj6vqybP4mYD5UlXHV9XGqvqKTyLz85jqnquqOj7JW5IsJHnXGOOvpvZizLWq2i/JfmOM86pqjyTnJnnyGOPiqjooybuSrE/ysDHGtVX1r0k2jTFeXVX7JNmYZN8xxk1bbHOvJF9JcuAY44c7/IcC5kZVLSTZlOTdSU5JcnaSp40xLp7pYMylqe25mvyi/l2S30pyVJKnVdVR03o95tsY46oxxnmT2zck2ZCfnF/1N0n+JMmW7wQekWSPqqoka5Ncl+TmJZs9MclHhBWwDR6ZxTdjT5m8STstixcuhu02zcOCj0zylTHGJX5R2R5VtS7JQ5J8rqpOSHLFGOOLS1a7JsmRSa5MckGSl44xll6W4aQk75/utMAycUB+8mnkJPlGfvIGD7bL1A4LVtWJSY4fYzxncv9ZSX55jPGiJeudnOTkJNl9TT1s/f1XTWUe5sMtt4xsumRz9r3vQu69x4ps/OrmHHHoLllYqFyw4cYcefiqrFxZ+fo3NmfFisqB+y3kxpuSL19yU446YlUWFipJsnnzyMWbbsqDjlqVxZ1b3JNtOn/NrEdgJ7c5N+WWbM5CVmaX7JrNuTG35JbsFr87bN0N+c61Y4x9li6f+aUYxhinJjk1SR7+4N3GWR87aMYTMSubN4888VlX5pkn7pGXP2/PXLDhxvzGU6/Md65f3CF1y63Jdd+9NZ/9yIF5ziuuyatftGce+6jVSZJjT7wif/m6++SRD9ktSfLWd343F228KX9/yn1n9vOw8zhu/6NnPQI7ue+Ob+eSXJyH1mOTJJeOLyVJDqn1sxyLndwnxulLL26dZLqHBa9IsmUpHThZBj9ljJHnvOKaHHn4qrz8eXsmSR545K755oWH5JKz1+WSs9flwP1W5pz/PCj73ndlDj5gZf7r04unUl39rZuz8as35dCDd7l9e6f92w056XfWzuRnAebPvbJnfpTv50fjB7l13Jqrc3n2yX6zHos5Nc09V2cnObyqDsliVJ2U5OlTfD3m2Jln/TjvPf2GPPDIVXnosZclSd742vvk8b+++52u//qX75Vnv/TqPPiYyzJG8qbX7Z2977OQJPna5Ztz+ZU359cevXqHzQ/MtxW1Ig8YR+fz+VRGRvbPuqyte896LObUtC/F8Pgkf5vFSzG8Z4zxF3e1vsOCwDQ4LAhMwyfG6eeOMR6+dPlUz7kaY3w4yYen+RoAADsTV2gHAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABrdbVxV1RFV9cmqunBy/0FV9frpjwYAMH+2Zc/VO5O8NsnmJBljnJ/kpGkOBQAwr7YlrtaMMc5asuzmaQwDADDvtiWurq2qw5KMJKmqE5NcNdWpAADm1MptWOeFSU5Nsr6qrkhyaZJnTnUqAIA5dbdxNca4JMmxVbV7khVjjBumPxYAwHy627iqqj9dcj9JMsb48ynNBAAwt7blsOAPtri9W5InJNkwjWE2nb8mx+1/9DQ2DdyDXfvcR896BGA5esfpd7p4Ww4L/vWW96vqlCQf65kKAGB5+Vmu0L4myYHdgwAALAfbcs7VBZlchiHJQpJ9kjjfCgDgTmzLOVdP2OL2zUmuHmO4iCgAwJ24y7iqqoUkHxtjrN9B8wAAzLW7POdqjHFLko1VdfAOmgcAYK5ty2HBPZNcVFVnZYvLMowxnjS1qQAA5tS2xNVt17a6TSV583TGAQCYb9sSVyvHGP+z5YKqWj2leQAA5tpW46qqnp/kBUkOrarzt3hojyRnTnswAIB5dFd7rv4pyUeSvCnJa7ZYfsMY47qpTgUAMKe2GldjjOuTXJ/kaTtuHACA+fazfP0NAABbIa4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARuIKAKCRuAIAaCSuAAAaiSsAgEbiCgCgkbgCAGgkrgAAGokrAIBG4goAoJG4AgBoJK4AABqJKwCARitnPQBsr2vHN7MpX8jIyAE5JOtq/axHAubQZWeclu99fUNWrl6b9b//x0mSq876SK7/2kVJVXZZvTYHH3NSdtn93jOelHkztT1XVfWeqrqmqi6c1mtwzzPGyMZ8PkfnV/PoHJdv5vJ8f3xv1mMBc2ivBzwih/72H91h2X2PPibrf+9VWf/UV+Zev3BUvnnux2c0HfNsmocF/yHJ8VPcPvdA1+e6rM7arKm1WVErcr8clG/lylmPBcyhtfsfloVd19xh2cKq3W6/fevmm3b0SCwTUzssOMb436paN63tc890Y36U3bL69vu7ZXWuz3UznAhYbq763Idz3aZzsrBqde7/pOfPehzmUI0xprfxxbj60Bjjl+5inZOTnDy5+4AkG6c2EMvBnknuleQHSa5NsleStUkum+VQwNxaleTwJBdN7u+dxb9bkmTfLB7hsXucrfmFMcY+SxfOPK5ge1TVo5P8WZL7jDEeXlWvTZIxxptmOhgwl5b+O1VV54wxHj65fXCSD/s3jO3lUgzMm7Oz+C5zVVWtSnJSkg/OdiRgGdl1i9snJPnSrAZhfrkUA3NljHFzVb0oyb8k2ZDkPWOMi+7maQA/paren+RxSfauqm8keUOSAyefcr81ydeTPG92EzKvpnZYcMtf2iRXJ3nDGOPdU3kx7nGq6uQxxqmzngNYXvzdQoepnnMFMCtV9f0xxtqq2j/JW8cYJ97Fui9LcuoY44fbsf3HJXnVGOMJP/+0wHLinCtgblTVwvY+Z4xx5V2F1cTLkqy5m3UAtom4AnYKVbWuqr5UVe+rqg1VdXpVramqr1XVm6vqvCRPrarDquqjVXVuVX2qavH7j6rqkKr6TFVdUFVvXLLdCye3F6rqlKq6sKrOr6oXV9VLkuyf5IyqOmOy3m9OtnVeVX2gqtZOlh8/mfG8JE/Z0f+PgPkgroCdyQOSvG2McWSS7yV5wWT5t8cYDx1jnJbk1CQvHmM8LMmrkrxtss5bkrx9jPHAJFdtZfsnJ1mX5OgxxoOSvG+M8dYsXsfomDHGMVW1d5LXJzl2jPHQJOckeUVV7ZbknUmemORhWbwGEsBP8WlBYGdy+RjjzMnt9yZ5yeT2PyfJZA/SryT5QFXd9pzbPjr/mCS/O7n9j0nefCfbPzbJO8YYNyfJGOPOLu//qCRHJTlz8hqrknwmyfokl44xvjyZ5b35yQWQAW4nroCdydJP2Nx2/weT/65I8t0xxtHb+PyfRSX5+BjjaXdYWLW11wS4A4cFgZ3JwZOr8CfJ05N8essHxxjfS3JpVT01SWrRgycPn5nFi8omyTO2sv2PJ3luVa2cPH+vyfIbkuwxuf3ZJI+pqvtP1tm9qo7I4sUk11XVYZP17hBfALcRV8DOZGOSF1bVhix+j+Tb72SdZyT5w6r6Yha/D+6EyfKXTp57QZIDtrL9d2XxeyjPnzz/6ZPlpyb5aFWdMcb4VpI/SPL+qjo/k0OCY4wfZ/Ew4H9MTmi/5uf7UYHlynWugJ2C7yIFlgt7rgAAGtlzBQDQyJ4rAIBG4goAoJG4AgBoJK4AABqJKwCARv8PBxkHwd3dmlIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import matplotlib.colors\n",
    "from sklearn.metrics import precision_recall_curve,\\\n",
    "                            average_precision_score,\\\n",
    "                            roc_auc_score, roc_curve,\\\n",
    "                            confusion_matrix, classification_report\n",
    "\n",
    "def plot_confusion_matrix2(train_labels, train_pred):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot()\n",
    "\n",
    "    labels = list(train_labels['Class'].value_counts().index)\n",
    " \n",
    "    confusion = confusion_matrix(train_labels, train_pred, labels=labels)\n",
    "    \n",
    "    print(confusion)\n",
    "    \n",
    "    ax.matshow(np.log(confusion + 1.001))\n",
    "\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_yticks(range(len(labels)))\n",
    "\n",
    "    ax.set_xticklabels(labels, rotation=90);\n",
    "    ax.set_yticklabels(labels);\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):        \n",
    "            ax.text(j, i, confusion[i,j], va='center', ha='center')\n",
    "\n",
    "    plt.xlabel('predicted')    \n",
    "    plt.ylabel('actual')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "#_=plot_confusion_matrix2(target_train_pd, model.predict(features_train_pd))\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix test set:\")\n",
    "_=plot_confusion_matrix2(target_test_pd, model.predict(features_test_pd))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Model to S3 Ceph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "key = \"model-new.pkl\"\n",
    "s3.upload_file(Bucket=s3_bucket, Key=key, Filename=\"model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
